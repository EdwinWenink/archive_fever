---
title: "Deepfakes and democracy: a case for technological mediation"
author: Edwin Wenink
date: 2019-10-12
toc: true
tags: [philosophy, deepfake, democracy, technology]
---

## Introduction

Recent successes in the production of so-called “deep fakes” sparked
both the imagination and the fears of many. The word “deepfake” is a
contraction of “deep learning” and “fake”, indicating the use of
Artificial Intelligence (AI) to synthesize images and videos that are
not real, while simultaneously not or barely being recognizable as
fabricated. For example, the recently launched website
thispersondoesnotexist.com [12] by Philip Wang showcases
AI-generated non-existing faces that are extremely realistic. Notably,
the underlying neural network technique based on Generative Adversial
Networks (GANs) is published [8] and publicly available -
including code - to those who are interested in implementing similar
applications. Currently, an app called *FakeApp* is being developed with
the goal to make the “technology available to people without a technical
background or programming experience.”[2]. At the same time,
there are serious concerns that as this technology becomes even better,
not only images but also videos can be completely faked. In the current
state-of-the-art it is already possible to “face swap” existing faces in
videos, allowing for example the face of President Trump to be inserted
in an arbitrary video. Despite leading to some very entertaining videos,
this technology is simultaneously a next step in the production of fake
news and has the potential to thoroughly disrupt democratic discourse.

In this essay I first highlight main threats of deepfakes to democratic
discourse. I claim that what these threats have in common is that they
result from a deepfake’s potential to mediate what we perceive to be
“real”. Secondly, I discuss how awareness of these negative societal
consequences elicits different stances towards the underlying
AI-technology, in particular concerning the responsibility that
developers have in openly publishing (or not) these technologies.
Thirdly, I argue that a philosophy of technological mediation is not
only an adequate framework for understanding how deepfakes threaten
democratic discourse through mediating what is “real”, but also for
expressing the full complexity of the question who is responsible for
negative societal consequences.

## Deepfakes disrupting democratic discourse

Societally undesirable applications of deepfake technology have already
emerged, and more negative consequences are anticipated to emerge as the
technology matures. One major negative application threatening
*individuals* is the creation of fake porn videos of celebrities, which
are now actively being banned from reddit and porn sites as they amount
to non-consentual porn [2] [1, p.18]. But on a
*societal* level, there are major concerns that deep fakes might
significantly disturb the type of political discourse that is essential
for democracy to function. Bobby Chesney and Danielle Citron
[1] are the first to extensively explore the relationship
between deep fakes and democratic discourse. Deepfakes first of all
enlarge threats to democracy that are already present in what some
consider to be a “post-truth” era, in which fake news can be as
effective for achieving political goals as actual news based on facts.
This threatens democratic discourse because, as Chesney et al.
adequately express: “One of the prerequisites for democratic discourse
is a shared universe of facts and truths supported by empirical
evidence. In the absence of an agreed upon reality, efforts to solve
national and global problems will become enmeshed in needless firstorder
questions like whether climate change is real. The large scale erosion
of public faith in data and statistics has led us to a point where the
simple introduction of empirical evidence can alienate those who have
come to view statistics as elitist.” [1, p.21]. Deepfakes in
this sense contribute to what Chesney et al. call intellectual vandalism
in the marketplace of ideas [1, p.21]. That development is
undesirable for democracy irrespective of its particular form, but is
particularly worrisome for those supporting a pluralist or deliberative
democracy, as they see opinion forming in a free and open dialogue or
debate as essential to democracy [9, 4-5].

But secondly, deep fakes can even more effectively undermine fair and
democratic intellectual competition in this marketplace of ideas than
“normal” fake news does. Imagine a deepfake video spreading on the
evening before elections, showing one of the candidates committing a
serious crime. Due to the power of social media such a video can go
“viral” and do serious damage to the eligibility of a candidate. In
modern media “not guilty until proven otherwise” often hardly holds, and
one can be convicted in the public eye for a crime that was not
committed, without fair trial. A well timed deep fake can heavily
disrupt fair democratic elections in this manner before there is a
chance to debunk the deepfake. But even if a deepfake is exposed as
false, its disruption of fair elections can still be effective by having
set a cognitive bias in the minds of the electorate [1, p.19].

Using deepfakes to disrupt democratic discourse will be even more
effective if they target situations that are already extremely tense.
Imagine for example a deepfake of “an Israeli official doing or saying
something so inflammatory as to cause riots in neighboring countries,
potentially disrupting diplomatic ties or sparking a wave of violence.”
[1, p.20]. Once such a situation is escalated, despite the
cause being “fake news”, it is extremely hard to de-escalate them. In
contexts where such distrust is already present, deep fakes can further
erode trust in institutions of open democratic discourse. As Chesney et
al. point out, in such tense situations the likelihood that opposing
camps will believe negative fake news about the other side is higher,
and only increases as deepfakes exploit this mechanism to further
enlarge social divisions [1, p.23]. Not surprisingly,
techniques to detect deepfakes are being developed to counteract these
risks, for example by the US military DARPA [4]. But due to
the flexibility of GAN neural networks it is likely that whatever
technology is developed in detecting fake videos might also be used as a
feedback mechanism, ultimately only improving the quality of deepfakes
[4]. These examples show that combatting the threats of
deepfake technology to democracy cannot be an exclusively technological
story. Despite technological counter-measures, deepfakes still threaten
democracy by setting cognitive biases and eroding a commonly agreed upon
reality that serves as the background for a meaningful democratic
dialogue. I argue in this essay that the mentioned threats to democratic
discourse are grounded in a deepfake’s potential to mediate what humans
perceive to be “real”. Furthermore, through mediating what is “real”,
deepfake artefacts can co-determine human praxis. Because of how
fundamental this theme is, I think we also need a philosophical story to
understand the impact of deepfakes. In the following sections I first
explore two diametrically opposed ways of coping with the societal
impact of deepfakes. I then show how a theory of technological mediation
is an appropriate philosophical framework for understanding this impact,
and moreover that it is able to grasp the complexity of the question how
to bear responsibility for it.

## Stances on technological disclosure

When one develops a technology that has a large societal impact, a quite
fundamental ethical question is to what extent the developer is
responsible for that impact. Philip Wang of *thispersondoesnotexist*
justifies promoting the GAN technique used for deepfakes in an interview
by pointing out that those “who are unaware are most vulnerable to this
technology” [7]. This taps into what can be called a
deterministic view on technology, which lets societal necessity follow
quite automatically from technological potentiality with the motto: “if
it *can* be done, it *will* be done”. In the field of AI deterministic
attitudes are well represented as AI-technology is increasingly changing
society. To the deterministic-minded person even those who worry about
these societal changes and remind us of the dangers, are nevertheless
equally subjected to the great historical impetus of technological
progression. And this person then reasons: if the technology will emerge
in society at some point in any case, then the best thing we can do is
raise awareness so we, as a society, can adapt to the technology -
rather than adapting the technology to human needs.

At the same time, other developers of AI-technology share the concerns
for its potential negative societal impact, but conceive of their own
responsibility differently. For example, the OpenAI research
organization dedicated to making sure AI benefits humanity, announced
last month that they developed an AI that can write paragraphs of text
that “feel close to human quality and show coherence over a page or more
of text” [6]. However, contrary to the publications about
video deepfakes, the OpenAI organization decided not to release the used
datasets, nor the trained model or the used code, due “to concerns about
large language models being used to generate deceptive, biased, or
abusive language at scale” amongst other “malicious applications of the
technology” [6]. At the same time, since the made technical
innovations “are core to fundamental artificial intelligence research”
and because they do not want to counteract progression of the field,
they released a smaller trained model with less potential for abuse as
an experiment in responsible disclosure in the field of AI
[6]. This is a more instrumentalist view on technology: its
development is controlled by humans, instead of being an autonomous
deterministic force to which humans have to adapt. The primary hope of
the decision to withhold the AI is that this will give the AI community
as well as governments more time to come up with ways to prevent or
penalize malicious use of AI technologies, quite similar to the practice of responsible
disclosure in cryptography, where organizations are given time to repair
security weaknesses before they are publicized. Interestingly, OpenAI's
explicit concern for the societal impact of their technology is framed
in the context of political actors waging “disinformation campaigns” by
generating fake content, requiring that “the public at large will need
to become more skeptical of text they find online, just as the “deep
fakes” phenomenon calls for more skepticism about images” [6].
In their policy OpenAI thus explicitly respond to the media attention
surrounding deepfake neural networks, as they become better at deceiving
people and are increasingly publicly available. Although not free of some hint of
determinism, the OpenAI initiative exerts a responsibility for actively
controlling technological development in AI, to make sure that it brings
forth useful instruments that are to the benefit and not the detriment
of humanity .

The contrast in the positions between a) the open publishing of deep
fake technology including trained models and code, and b) the controlled
disclosure of text-generating networks, again shows that the development
of these technologies does not only raise technical issues, but also
societal ones. In both cases, the researchers are aware of the societal
dangers of their technology, but take responsibility for it in different
ways. In a deterministic vein, there is no reason to control disclosure
of technology: someone else will do it anyways, and it is better to
inform people as soon as possible. From a more instrumentalist point of
view, the act of disclosure is not as neutral: since humans have at
least some control over technology, they also share responsibility for
possible negative consequences within reasonable limits. After all, the
technology itself is just a neutral instrument. Whether it is put to
good use depends on humans.

But what both views have in common is that they conceptualize the
human-technological relationship in terms of a subject-object divide in
which subject and object are external to each other, irrespective of
whether the subject is human or some technology. But I think that these
terms are no longer sufficient for understanding the complexity of
deepfakes that heavily blur the demarcation between what is “real” and
what is not, and consequently also not for understanding how this is the
foundation of a threat to democracy. Accordingly, if we are to conceptualize
the responsibilities of developers of such technologies, we need to take
into account how these technologies mediate reality and human praxis.

## Deepfakes and Technological Mediation

In this section I argue that the philosophy of technological mediation
as put forward by Verbeek [11][10] is appropriate
for conceptualizing the threat of deepfakes to democracy through their
mediation of human praxis. Technological mediation “concerns the role of
technology in human action (conceived as the ways in which human beings
are present in their world) and human experience (conceived as the ways
in which their world is present to them)” [10, p.363]. That
technological artefacts *mediate* means that they “are not neutral
intermediaries but actively coshape people’s being in the world”, and do
so in two directions: they mediate how the world appears to humans
(perception) and how humans give shape to their own reality by acting in
the world through the use of technological artefacts (praxis)
[10, p.364]. The mediation of deepfakes can be shown in both
directions, and I will indicate how they are interrelated in the example
of democracy.

First of all, what the name “deepfake” expresses is that a given image
or video is perceived to be “real”, while what is represented does not
exist in the represented capacity: i.e. it is “fake”. I chose this
specific formulation because a deep fake of Trump does not necessarily
mean that Trump does not exist, but merely that he did not say or do
what is represented in the deep fake video or image.

Now imagine a video of a man committing a serious crime, with the face
of Trump swapped in. In case of a successful deepfake, we do not see a
man with Trump’s face superimposed. Instead we perceive this man *as*
Trump. The “as” in that sentence indicates an important insight from
hermeneutic philosophy: the beings in our world always already appear to
us as meaningful in a quite practical sense. The stereotypical example,
based on Heidegger’s early philosophy, is that we see a hammer not as a
composite object with one wood handle and one metal head, but
intuitively and immediately take it *as* something we can hit nails with
[10, cf. p.364]. Philosophical hermeneutics regards this as an
act of interpretation that is not some scholarly exercise, but one that
quite fundamentally determines how beings become present to us in the
context of a world [cf. 5]. The particularity of
deepfakes is that their technology *mediates* this process by making us
pre-reflectively take something “fake” *as* something “real”. What is
important is that, against instrumentalism, a deepfake’s deceiving
character is not simply due to the bad intention of its designer. The
technology itself is not a completely neutral tool in the theory of
technological mediation. As it helps to shape what counts as “real”,
this technology quite fundamentally sets a horizon for human moral
and/or political action. Instead, mixing up fiction and reality is a
core feature of the GAN technology that actively influences the
relationship between a human and its world. A deepfake can thus be said
to have its own “technological intentionality” [10 p.456] that
affords (not causes!) the interpretation of “fake” as “real”.

But against determinism, this technological intentionality does not
imply that the technological artefact autonomously decides our social
realities, as if the technological artefact takes care of its own
interpretation. As Verbeek makes clear, following Don Ihde, this
technological intentionality only takes form in the interaction with
humans [10 p.456]. Stating that technological intentionality
does not coincide with human intentionality is analogue to the
hermeneutic insight that the meaning of a text is not equal to the
intention of its author. Despite this independence from the author’s
intention however, it is equally naive in hermeneutics to say that the
meaning of a text resides solely in the text itself as some pure ideal
content, which would then be the same and equally complete even if
nobody ever read it. Instead, and herein lies the analogue, a text’s
meaning unfolds in the interaction with a reader. With respect to
deepfake technology, this also means that its effects cannot be fully
predicted independent of any real world interaction of humans with
deepfake artefacts. I argue that in this manner a deepfake mediates how we perceive
beings in the world by affording an interpretation of the fake as the
real. If effective, a deepfake is not seen as just a video, but as
representing an event in the world as we perceive it around us. *But
this interpretative step is everything but neutral*. If we revisit the
example of a deepfake of Trump performing a criminal act, we can see
that this does not only imply we perceive the criminal *as* Trump, but
that it also implies we now might perceive Trump *as* a criminal. We can
then see how the hermeneutical effect of deepfakes underlies its effects
in praxis:

*If the fake is interpreted as real, then the real is reinterpreted in
terms of the fake*.

So if a candidate for a democratic election is shown in a deepfake to
perform e.g. criminal acts (something fake is interpreted as real), then
this candidate is potentially reinterpreted and reassessed by citizens
as if he were a criminal (the real interpreted in terms of the fake).
The aforementioned cognitive bias could also be interpreted along these
lines: it is a re-valuation of something in the world because the
deepfake artefact meddled with the interpretative process by which we
take something *as* something.

Deepfakes thus contribute to the further blurring of the demarcation
between real and fake news. As a result, even real and genuine discourse
can become suspect, as it is now fair game to the question “fake or
real?” But can we then still establish what we said was necessary for
democracy? Can we in the future still have the certainty of an agreed
upon reality, on the basis of which we can have a meaningful dialogue in
the marketplace of ideas within a democracy?

## Conclusion

I have argued that the threat of deepfakes to democracy can be framed in
terms of technological mediation, as we have regarded serious threats to
democracy as a result of interpreting something fake as real. That means
that deepfake technological artefacts can mediate both the (hermeneutic)
experience of the surrounding world, and the actions humans take in it.
But the perspective of technological mediation only makes the question
who is responsible for (unintended) negative consequences more complex.
One the one hand, developers of these technologies cannot be held fully
responsible for negative consequences of technology, because they cannot
fully predict how the interaction with users works out. But neither can
developers realistically waive all responsibility by claiming that the
development of AI is a historical movement shaping our social realities
independent of human interaction. Instead, when AI increasingly changes
our social and political reality in unexpected ways, the more accurate
position is admitting that somehow responsibility is distributed between
developers, the technology itself, and its users. And especially if AI systems
take on more autonomy in the future, the question of sharing
responsibility with moral machines becomes increasingly urgent and
intriguing.

Although such an open conclusion is not satisfying, it is the more
honest position. When it comes to the *moral* responsibility (rather
than a more limited legalistic story), issues around deepfakes can join
the ranks of complicated ongoing debates about ethical responsibility in
accidents with self-driving cars, or killer drones. The unresolved
paradox is that due to the flexibility of AI unforeseen negative
consequences may occur, whereas at the same time, *this flexibility is
programmed and exactly the main innovation of state-of-the-art AIs*. And
yet, we can reasonably ask of developers to foresee certain undesirable
applications of their technologies. From the viewpoint of technological
mediation both the stances of Philip Wang and of the OpenAI foundation
have their own place. The decision of OpenAI to withhold their AI
technology results from a reasonable anticipation of negative
consequences, awaiting further democratic discussion before full
disclosure. At the same time, this attitude should not tip the balance
towards censorship. Withholding a technology from society in order to
protect democracy seems paradoxically undemocratic and patronizing if
not based on a sustained debate. Informing the general population about
the threats of a technology is also desirable, but should not depart
from a deterministic motivation. It is good, not because we have to
learn to adapt to an uncompromising technology, but to spark a
democratic debate with all involved stakeholders about how to design a
better interaction with the technology [cf. 10].

## References
<br>

<p style="text-align:left">[1]: Robert Chesney and Danielle Keats Citron. Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. SSRN Electronic Journal, 2018. doi: 10.2139/ssrn.3213954.</p>

<p style="text-align:left">[2] Samantha Cole. We are truly fucked: Everyone is making ai-generated fake porn now, 2018. URL https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley. (accessed: 2019- 03-21).</p>

<p style="text-align:left">[3] Maarten Franssen, Gert-Jan Lokhorst, and Ibo van de Poel. Philosophy of technology. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, fall 2018 edition, 2018. URL https://plato.stanford.edu/archives/fall2018/entries/technology/. (accessed: 2019-03-27).</p>

<p style="text-align:left">[4] Will Knight. The defense department has produced the first tools for catching deepfakes, 2019. URL https://www.technologyreview.com/s/611726/the-defense-department-has-produced-the-first-tools-for-catching-deepfakes/. (accessed: 2019-03-23).</p>

<p style="text-align:left">[5] C. Mantzavinos. Hermeneutics. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter 2016 edition, 2016. URL https://plato.stanford.edu/archives/win2016/entries/hermeneutics/. (accessed: 2019-03-27).</p>

<p style="text-align:left">[6] OpenAI. Better language models and their implications, 2019. URL https://openai.com/blog/better-language-models.</p>

<p style="text-align:left">[7] Danny Paez. 'this person does not exist' creator reveals his site's creepy origin story, 2019. URL https://www.inverse.com/article/53414-this-person-does-not-exist-creator-interview. (accessed: 2019-03-21).</p>

<p style="text-align:left">[8] Timo Aila Tero Karras, Samuli Laine. A style-based generator architecture for generative adversarial networks, 2019. URL https://arxiv.org/abs/1812.04948. (accessed: 2019-03-21).</p>

<p style="text-align:left">[9] Jan A. G. M. Van Dijk. Digital democracy: Vision and reality. Innovation and the Public Sector, 19:49-62, 2012. doi: 10.3233/978-1-61499-137-3-49.</p>

<p style="text-align:left">[10] Peter-Paul Verbeek. Materializing morality: Design ethics and technological mediation. Science, Technology, & Human Values, 31(3):361-380, 2006. doi: 10.1177/0162243905285847. URL https://doi.org/10.1177/ 0162243905285847.</p> 

<p style="text-align:left">[11] Peter-Paul Verbeek. Mediation theory. 2019. URL https://ppverbeek.wordpress.com/mediation-theory/. (accessed: 2019-03-23).</p>

<p style="text-align:left">[12] Philip Wang. Thispersondoesnotexist, 2019. URL https://thispersondoesnotexist.com/. (accessed: 2019- 03-21). </p>
